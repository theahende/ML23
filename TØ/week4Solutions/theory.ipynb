{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 (Theoretical) Exercises\n",
    "Remember to take a look at all the exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Convex Functions \n",
    "\n",
    "See https://en.wikipedia.org/wiki/Convex_function for the definitions of convex functions.\n",
    "At the lectures we saw two definitions. But there is a third one that is usually easier and is as follows.\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a function that is twice differentiable.\n",
    "\n",
    "Letting $x=(x_1,\\dots,x_n)$ denote the input variables to $f$, we define the Hessian $H(x)$ as the matrix of second order derivatives as (a matrix of shape $n \\times n$):\n",
    "$$H(x) = \\left[ \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\right]_{i,j}$$\n",
    "\n",
    "A function f is convex in a convex set $X \\subseteq \\mathbb{R}^n$ if $H(x)$ is a Positive Semidefinite Matrix (PSD) for every $x \\in X$. A matrix $A$ is PSD if $\\forall y\\in \\mathbb{R}^n$,  $ y^\\intercal A y \\geq 0$. \n",
    "\n",
    "For a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ the Hessian is $1 \\times 1$ and looks like $H(x) = f''(x)$. The condition then says that $\\forall y: y f''(x) y = y^2 f''(x) \\geq 0$. This is the case if and only if $f''(x) \\geq 0$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Which of the following functions are convex on ${\\mathbb R}$? \n",
    "\n",
    "-   $f(x) = 2$\n",
    "\n",
    "-   $f(x) = -\\ln (x), x>0$\n",
    "\n",
    "-   $f(x) = x^3$\n",
    "\n",
    "-   $f(x) = x^2 + x^4$\n",
    "\n",
    "\n",
    "**hint: use the newly defined definition to determine convexity**\n",
    "\n",
    "\n",
    "### solution math\n",
    "We compute the $f''(x)$ in all cases:\n",
    "\n",
    "$f(x)=2$ has $f'(x)=0$ and $f''(x)=0$. Hence convex.\n",
    "\n",
    "$f(x)=-\\ln(x)$ for $x>0$ has $f'(x) = -1/x$ and $f''(x) = 1/x^2$. Hence $f''(x)\\geq0$ for $x>0$ and thus convex.\n",
    "\n",
    "$f(x)=x^3$ has $f'(x)=3x^2$ and $f''(x) = 6x$. We have $f''(x)<0$ when $x<0$, hence not convex.\n",
    "\n",
    "$f(x)=x^2+x^4$ has $f'(x)=2x + 4x^3$ and $f''(x)=2 + 12x^2$. Since $x^2>0$ for all $x$, we have that $f$ is convex.\n",
    "### end solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Lets plot them\n",
    "x = np.linspace(-1, 1, 1000)\n",
    "xp = x[x>0]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x, [2 for y in x], 'r-', label='f(x)=2')\n",
    "plt.plot(xp, [-np.log(z) for z in xp], 'g-', label='f(x)=ln(x), x>0')\n",
    "plt.plot(x, x**3, 'b-', label='f(x)=x^3')\n",
    "plt.plot(x, x**2 + x**4, 'y-', label='f(x)=x^2+x^4')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Convexity of Linear Regression Cost Funtion\n",
    "In this exercise your job is to prove that the cost function for linear regression is convex.\n",
    "**Prove that the function $E_\\mathrm{in}(w) = \\frac{1}{n} \\|Xw -y\\|^2$ is convex.** \n",
    "\n",
    "\n",
    "\n",
    "- Hint 1: Prove that for all matrices $A$, it holds that $A^\\intercal A$ is Positive Semidefinite. \n",
    "- Hint 2: Compute the Hessian of $E_\\mathrm{in}(w)$ \n",
    "\n",
    "### solution math\n",
    "For a matrix $M=A^\\intercal A$, notice that for any vector $x$, we have $x^\\intercal M x = x^\\intercal A^\\intercal A x = \\|Ax\\|^2 > 0$.\n",
    "\n",
    "The Hessian is the matrix with entry $(i,j)$ equal to $\\frac{\\partial^2 E_\\mathrm{in}(w)}{\\partial w_i \\partial w_j}$. We see that \n",
    "\n",
    "$\\frac{\\partial^2 E_\\mathrm{in}(w)}{\\partial w_i \\partial w_j} = \\frac{1}{n} \\frac{ \\sum_{k}\\partial^2 (Xw-y)_k^2}{\\partial w_i \\partial w_j} = \\frac{1}{n} \\frac{ \\sum_{k}\\partial^2 (x_k^\\intercal w-y_k)^2}{\\partial w_i \\partial w_j} = \\frac{1}{n} \\frac{ \\sum_{k}\\partial 2(x_k^\\intercal w-y_k)x_{k,i}}{\\partial w_j} = \\frac{2}{n} \\sum_{k} x_{k,j} x_{k,i} = \\frac{2}{n} x_{.,i}^\\intercal x_{.,j}$.\n",
    "\n",
    "And therefore\n",
    "\n",
    "$H = \\frac{2}{n}X^\\intercal X$ which we know is PSD as the factor $2/n$ does not change the fact that $x^\\intercal H x \\geq 0$ for all $x$.\n",
    "### end solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Gradient Descent \n",
    "Let $f_a(x_1, x_2) = \\frac{1}{2}(x_1^2 + a\\cdot x_2^2)$\n",
    "\n",
    "Where $a$ is parameter that we will change.\n",
    "\n",
    "**Your task is to write a gradient descent algorithm that finds a minimizer of $f$, where we have decided that the starting point for the gradient descent is (256, 1). It should be possible for you to figure out what the local (global) minimum is, as well as the gradient.**\n",
    "- Test your algorithm by running the cell.\n",
    "- run your gradient descent algorithm for at least 40 steps to see if it converges. \n",
    "You must save the sequence of elements (2d points) considered in your gradient descent algorithm for visualization. \n",
    "We have added code to visualize this sequence.\n",
    "\n",
    "- Try a=1, 4, 16, 64, 128, 256 and adjust the step size to see if you can make it converge.\n",
    "    **hint - after trying different values for the stepsize also try approximately 1/a (for a > 1)**\n",
    "- What do you see? \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(a, x):\n",
    "    return 0.5 * (x[0]**2 + a * x[1]**2)\n",
    "\n",
    "def visualize(a, path, ax=None):\n",
    "    \"\"\"\n",
    "    Make contour plot of f_a and plot the path on top of it\n",
    "    \"\"\"\n",
    "    y_range = 10\n",
    "    x = np.arange(-257, 257, 0.1)\n",
    "    y = np.arange(-y_range, y_range, 0.1)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = 0.5 * (xx**2 + a * yy**2)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(16, 13))\n",
    "    h = ax.contourf(xx, yy, z, cmap=plt.get_cmap('jet'))\n",
    "    ax.plot([x[0] for x in path], [x[1] for x in path], 'w.--', markersize=4)\n",
    "    ax.plot([0], [0], 'rs', markersize=8) # optimal solution\n",
    "    ax.set_xlim([-257, 257])\n",
    "    ax.set_ylim([-y_range, y_range])\n",
    "\n",
    "def gd(a, step_size=0.1, steps=40):\n",
    "    \"\"\" Run Gradient descent\n",
    "        params:\n",
    "        a - the parameter that define the function f\n",
    "        step_size - constant stepsize to use for gradient descent\n",
    "        steps - number of steps to run\n",
    "        \n",
    "        Returns: out, list with the sequence of points considered during the descent.         \n",
    "    \"\"\"\n",
    "    out = []\n",
    "    x = np.array([256.0, 1.0]) # starting point\n",
    "\n",
    "    ### YOUR CODE HERE    \n",
    "    def nablaf(a, x):\n",
    "        return np.array((x[0],a * x[1]))\n",
    "    # the point is simply that number of gs steps depend on function\n",
    "    #step_size = min(2/a, 0.1)\n",
    "    for i in range(0,steps):\n",
    "        grad = nablaf(a,x)\n",
    "        x = x - step_size*grad\n",
    "        out.append(x)\n",
    "    ### END CODE\n",
    "    return out\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 16))\n",
    "ateam = [[1, 4, 16], [64, 128, 256]]\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax = axes[i][j]\n",
    "        a = ateam[i][j]\n",
    "        path = gd(a, step_size=0.1, steps=40) # use good step size here instead of standard value\n",
    "        visualize(a, path, ax)\n",
    "        ax.set_title('Gradient Descent a={0}'.format(a), fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4:  Show that the cost function for Logistic Regression is convex\n",
    "### Try at least the first part. Do the second part after coding exercises if time.\n",
    "\n",
    "In class we derived the loss function for logistic regression based on negative log likelihood. The loss function was as follows:\n",
    "$$\n",
    "E_{in}(w) = - \\frac{1}{n}\\sum_{i=1}^n \\ln(\\sigma(y_i w^\\intercal x_i)) = \\frac{1}{n}\\sum_{i=1}^n \\ln(1 + e^{-y_i w^\\intercal x_i})\n",
    "$$\n",
    "where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function with derivative $\\frac{\\partial \\sigma}{\\partial z} = \\sigma(z)\\sigma(-z)$.\n",
    "\n",
    "\n",
    "We need to prove that $E_{in}(w)$ is a convex function (data X, y fixed as usual).\n",
    "A sum of convex functions is convex so we can ignore the sum and focus on just one element. Similarly, the factor $1/n$ does not change convexity. Thus it suffices to show that\n",
    "$$\n",
    "f(w) = -\\ln(\\sigma(y w^\\intercal x))\n",
    "$$\n",
    "is a convex function (of $w$) for any fixed $x$ and $y$.\n",
    "\n",
    "\n",
    "\n",
    "We will do this in simple steps. First let us assume that x and w are 1D vectors i.e. numbers.\n",
    "To prove that $f$ is convex we can prove that $f''(w) \\geq 0$ for all $w$.\n",
    "* Step 1. Prove that $f'(w) = -\\sigma(-ywx)yx$.\n",
    "* Step 2. Prove that $f''(w) =  x^2 \\sigma(yw x)\\sigma (-yw x)$.\n",
    "* Step 3. Argue that $f''(w) \\geq 0$ for all w\n",
    "\n",
    "\n",
    "To generalize this to d-dimensional $w$ and $x$, we do the same steps, just with partial derivates. \n",
    "\n",
    "* Step 1. Show that $\\partial f/\\partial w_i = -\\sigma(-yw^\\intercal x)yx_i$.\n",
    "\n",
    "The Hessian matrix is the second order matrix of derivatives.\n",
    "\n",
    "* Step 2. Show that the Hessian of f is $\\sigma(yw^\\intercal x) \\sigma(-yw^\\intercal x) x x^\\intercal$ (note that this is an outer product) \n",
    "\n",
    "* Step 3. Show that $\\sigma(yw^\\intercal x) \\sigma(-yw^\\intercal x) x x^\\intercal$ is a Positive Semidefinite Matrix for all $w, x$ and $y$.\n",
    "\n",
    "\n",
    "### solution math\n",
    "1D vectors.\n",
    "\n",
    "Step 1. Using the chain rule twice:\n",
    "$f'(w) = -\\sigma(ywx)^{-1} \\cdot (d/dw) \\sigma(ywx) = -\\sigma(ywx)^{-1} \\sigma(ywx) \\sigma(-ywx) (d/dw)(ywx)= -\\sigma(-ywx)yx$.\n",
    "\n",
    "Step 2. We compute the second derivative using the chain rule:\n",
    "\n",
    "$f''(w) = -yx\\sigma(-ywx)\\sigma(ywx) (d/dw)(-ywx) = y^2 x^2 \\sigma(ywx) \\sigma(-ywx) = x^2 \\sigma(ywx)\\sigma(-ywx)$.\n",
    "\n",
    "Step 3. We know that $x^2$ is non-negative for all $x$. We know that $\\sigma(z) \\in [0,1]$ for all $z$, hence $f''(w) \\geq 0$ for all $w$.\n",
    "\n",
    "Now with general vectors.\n",
    "\n",
    "Step 1. Repeating step 1. above, only with partial derivatives, only the last step changes: \n",
    "$\n",
    "\\partial f/\\partial w_i = -\\sigma(yw^\\intercal x)^{-1} \\cdot (\\partial/\\partial w_i) \\sigma(yw^\\intercal x) = -\\sigma(yw^\\intercal x)^{-1} \\sigma(yw^\\intercal x) \\sigma(-yw^\\intercal x) (\\partial/\\partial w_i)(yw^\\intercal x)= -\\sigma(-yw^\\intercal x)yx_i.\n",
    "$\n",
    "\n",
    "Step 2. We need to compute $\\partial^2 f/\\partial w_i \\partial w_j$. We already know that $\\partial f/\\partial w_i = -\\sigma(-yw^\\intercal x)yx_i$. We then get\n",
    "\n",
    "$\n",
    "\\partial^2 f/\\partial w_i \\partial w_j = (\\partial/\\partial w_j) -\\sigma(-yw^\\intercal x)yx_i = -y x_i \\sigma(-yw^\\intercal x) \\sigma(yw^\\intercal x) (\\partial/\\partial w_j) (-yw^\\intercal x) = x_i x_j  \\sigma(-yw^\\intercal x) \\sigma(yw^\\intercal x).\n",
    "$\n",
    "\n",
    "Thus the Hessian equals $\\sigma(yw^\\intercal x) \\sigma(-yw^\\intercal x) x x^\\intercal$.\n",
    "\n",
    "Step 3. Since $\\sigma(z) \\in [0,1]$ for all $z$, we know that $\\sigma(yw^\\intercal x) \\sigma(-yw^\\intercal x) > 0$. Thus we only need to argue that $xx^\\intercal$ is PSD. This is seen by observing that $y^\\intercal x x^\\intercal y = (x^\\intercal y)^2 \\geq 0$ for all $y$.\n",
    "### end solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS Exercise if Time 5: Softmax Gradient\n",
    "As described in the softmax note, we define the softmax function as follows:\n",
    "$$\n",
    "\\textrm{softmax}:\\mathbb{R}^K \\rightarrow \\mathbb{R}^K, \\quad\n",
    "\\textrm{softmax}(x)_i =\n",
    "\\frac{e^{x_i}}\n",
    "{\\sum_{j=1}^K e^{x_j}}\\quad\n",
    "\\textrm{ for }\\quad i = 1, \\dots, K.\n",
    "$$\n",
    "where  $\\textrm{softmax}(x)_i$ denote the $i$'th output of the function\n",
    "\n",
    "\n",
    "Show that the matrix of derivatives of the softmax function is as follows.\n",
    "$$\n",
    "\\left[\\frac{\\partial \\textrm{softmax}}{\\partial x}\\right]_{i,j} =\n",
    "\\frac\n",
    "{\\partial \\;\\textrm{softmax}(x)_i}\n",
    "{\\partial x_j} =\n",
    "(\\delta_{i,j} - \\textrm{softmax}(x)_j)\n",
    "\\textrm{softmax}(x)_i\\quad\\quad\\text{where}\\quad\\quad\n",
    "\\delta_{ij}=\\begin{cases}1 &\\text{if }i=j\\\\\n",
    "0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### solution math\n",
    "For $i\\neq j$\n",
    "$$ \\frac{\\partial \\textrm{softmax}(x)_i}\n",
    "{\\partial x_j} = e^{x_i} \\frac{-1} \n",
    "{\\left(\\sum_{a=1}^K e^{x_a}\\right)^2} e^{x_j} = -\\frac{e^{x_i}}{\\sum_{a=1}^K e^{x_a}}\\frac{e^{x_j}}{\\sum_{a=1}^K e^{x_a}} = -\\textrm{softmax}(x)_j \\textrm{softmax}(x)_i = (0 - \\textrm{softmax}(x)_j )\\textrm{softmax}(x)_i\n",
    "$$\n",
    "For $i=j$, we use that $(f \\cdot g)' = f'g + fg'$\n",
    "$$\\frac{\\partial \\textrm{softmax}(x)_j}\n",
    "{\\partial x_j} = \\textrm{softmax}(x)_j + e^{x_j} \\frac{-1} {\\left(\\sum_{a=1}^K e^{x_a}\\right)^2} e^{x_j} = \\textrm{softmax}(x)_j -  (\\textrm{softmax}(x)_j)^2 = (1 - \\textrm{softmax}(x)_j)\\textrm{softmax}(x)_j\n",
    "$$\n",
    "Combining the two derivations the result follows\n",
    "### end solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
